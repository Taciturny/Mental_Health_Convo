{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "084a05fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/run/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a405690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "# from transformers import \n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "# import os\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e593ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hea.parquet', 'Mental_Health_FAQ.csv', 'preprocessed_data.parquet']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('Mental_Hea/data/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "362794cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>8e16f86b-05bb-4b17-9473-6cc52a9f0534</td>\n",
       "      <td>i'm having trouble with my parents and i don't know how to communicate with them effectively.</td>\n",
       "      <td>let's work on communication skills to help you express your needs and feelings to your parents. would you be open to family therapy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4119</th>\n",
       "      <td>2158ff7a-2595-4a66-9a47-08ee6bbcd93a</td>\n",
       "      <td>i'm having trouble with my motivation and productivity.</td>\n",
       "      <td>motivation and productivity issues can be a common source of stress and anxiety, but it's important to address them before they become overwhelming. let's explore any underlying issues that may be contributing to your lack of motivation or productivity, and work on developing healthy habits and strategies to help you achieve your goals.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        id  \\\n",
       "2055  8e16f86b-05bb-4b17-9473-6cc52a9f0534   \n",
       "4119  2158ff7a-2595-4a66-9a47-08ee6bbcd93a   \n",
       "\n",
       "                                                                                           question  \\\n",
       "2055  i'm having trouble with my parents and i don't know how to communicate with them effectively.   \n",
       "4119                                        i'm having trouble with my motivation and productivity.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                  answer  \n",
       "2055                                                                                                                                                                                                                let's work on communication skills to help you express your needs and feelings to your parents. would you be open to family therapy?  \n",
       "4119  motivation and productivity issues can be a common source of stress and anxiety, but it's important to address them before they become overwhelming. let's explore any underlying issues that may be contributing to your lack of motivation or productivity, and work on developing healthy habits and strategies to help you achieve your goals.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.read_parquet('Mental_Hea/data/preprocessed_data.parquet')\n",
    "\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e4c3199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /run/cache/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token=os.environ['HUGGINGFACE_HUB_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82f42a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3d2f7daffb4899a44b81a935f42ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ModelSingleton:\n",
    "    _instance = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_instance():\n",
    "        if ModelSingleton._instance is None:\n",
    "            ModelSingleton()\n",
    "        return ModelSingleton._instance.model, ModelSingleton._instance.tokenizer\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(ModelSingleton, cls).__new__(cls)\n",
    "            cls._instance._initialize_model_and_tokenizer()\n",
    "        return cls._instance\n",
    "    \n",
    "    def _initialize_model_and_tokenizer(self):\n",
    "        model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Update to Mistral 7B\n",
    "        \n",
    "        # Retrieve the Hugging Face token from environment variables\n",
    "        token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "        if auto_token is None:\n",
    "            print(\"Warning: HUGGINGFACE_HUB_TOKEN is not set.\")\n",
    "        \n",
    "        # Configure quantization\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "        # Load the model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, quantization_config=quantization_config, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=token\n",
    "        )\n",
    "\n",
    "# Usage\n",
    "model, tokenizer = ModelSingleton.get_instance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36e4fb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "def generate_questions(context, answer, model, tokenizer):\n",
    "    # Define the prompt template in Mistral 7B v2 instruction format\n",
    "    prompt_template = (\n",
    "        \"<s>[INST] Context: {context}\\nAnswer: {answer}\\nPlease list 5 insightful questions based on this context and answer. [/INST]\"\n",
    "    )\n",
    "\n",
    "#     prompt_template = (\n",
    "#         \"<s>[INST] Given the following context and answer, generate 5 insightful questions that can be used for evaluating the ground truth:\\n\"\n",
    "#         \"Context: {context}\\n\"\n",
    "#         \"Answer: {answer}\\n\"\n",
    "#         \"Questions: [/INST]\"\n",
    "#     )\n",
    "    \n",
    "    # Format the prompt with the provided context and answer\n",
    "    prompt = prompt_template.format(context=context, answer=answer)\n",
    "    \n",
    "    # Tokenize the prompt and generate text\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    device = model.device\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    # Generate text using the model\n",
    "    outputs = model.generate(input_ids, max_length=512, num_return_sequences=1, do_sample=True, pad_token_id=tokenizer.eos_token_id )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract questions from the generated text\n",
    "    # Split generated text into lines\n",
    "    lines = generated_text.split('\\n')\n",
    "    \n",
    "    # Collect questions starting with numbers followed by a period\n",
    "    questions = []\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(f\"{len(questions) + 1}.\"):\n",
    "            question = line.strip().split('.', 1)[-1].strip()\n",
    "            questions.append(question)\n",
    "        if len(questions) >= 5:\n",
    "            break\n",
    "    \n",
    "    return questions\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer = ModelSingleton.get_instance()\n",
    "    \n",
    "    # Sample context and answer\n",
    "    context = \"I have low self-esteem and feel like I'm not good enough. What can I do to improve my self-confidence?\"\n",
    "    answer = \"It's important to challenge negative self-talk and practice self-care activities like exercise, spending time with loved ones, and engaging in hobbies. Additionally, seeking help from a therapist or mental health professional can provide support and guidance.\"\n",
    "    \n",
    "    questions = generate_questions(context, answer, model, tokenizer)\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"Question {i}: {question.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75d49de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 565, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 761, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 746, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 677, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 523, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 639, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 897, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 1308, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 533, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 573, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 963, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 678, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 577, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 622, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 1002, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def generate_questions_for_sample(df, model, tokenizer, n=500,  random_state=None):\n",
    "    questions_data = []\n",
    "\n",
    "    # Limit the DataFrame to the first n records\n",
    "    df_subset = df.sample(n=n, random_state=random_state)\n",
    "    \n",
    "    for _, row in df_subset.iterrows():\n",
    "        record_id = row['id']\n",
    "        context = row['question']\n",
    "        answer = row['answer']\n",
    "        \n",
    "        # Generate questions\n",
    "        questions = generate_questions(context, answer, model, tokenizer)\n",
    "        \n",
    "        # Append questions with their IDs\n",
    "        for question in questions:\n",
    "            questions_data.append({'id': record_id, 'question': question})\n",
    "    \n",
    "    return questions_data\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer = ModelSingleton.get_instance()\n",
    "\n",
    "    questions_data = generate_questions_for_sample(df, model, tokenizer)\n",
    "    \n",
    "    # Create a DataFrame from the questions_data and save it to a parquet file\n",
    "    questions_df = pd.DataFrame(questions_data)\n",
    "    questions_df.to_parquet('generated_questions.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a21de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
